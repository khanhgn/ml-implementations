{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"../datasets/logisitc_regression/framingham.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>male</th>\n",
       "      <th>age</th>\n",
       "      <th>education</th>\n",
       "      <th>currentSmoker</th>\n",
       "      <th>cigsPerDay</th>\n",
       "      <th>BPMeds</th>\n",
       "      <th>prevalentStroke</th>\n",
       "      <th>prevalentHyp</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>totChol</th>\n",
       "      <th>sysBP</th>\n",
       "      <th>diaBP</th>\n",
       "      <th>BMI</th>\n",
       "      <th>heartRate</th>\n",
       "      <th>glucose</th>\n",
       "      <th>TenYearCHD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>26.97</td>\n",
       "      <td>80.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>28.73</td>\n",
       "      <td>95.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>127.5</td>\n",
       "      <td>80.0</td>\n",
       "      <td>25.34</td>\n",
       "      <td>75.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>28.58</td>\n",
       "      <td>65.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>285.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>23.10</td>\n",
       "      <td>85.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   male  age  education  currentSmoker  cigsPerDay  BPMeds  prevalentStroke  \\\n",
       "0     1   39        4.0              0         0.0     0.0                0   \n",
       "1     0   46        2.0              0         0.0     0.0                0   \n",
       "2     1   48        1.0              1        20.0     0.0                0   \n",
       "3     0   61        3.0              1        30.0     0.0                0   \n",
       "4     0   46        3.0              1        23.0     0.0                0   \n",
       "\n",
       "   prevalentHyp  diabetes  totChol  sysBP  diaBP    BMI  heartRate  glucose  \\\n",
       "0             0         0    195.0  106.0   70.0  26.97       80.0     77.0   \n",
       "1             0         0    250.0  121.0   81.0  28.73       95.0     76.0   \n",
       "2             0         0    245.0  127.5   80.0  25.34       75.0     70.0   \n",
       "3             1         0    225.0  150.0   95.0  28.58       65.0    103.0   \n",
       "4             0         0    285.0  130.0   84.0  23.10       85.0     85.0   \n",
       "\n",
       "   TenYearCHD  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           1  \n",
       "4           0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>male</th>\n",
       "      <th>age</th>\n",
       "      <th>education</th>\n",
       "      <th>currentSmoker</th>\n",
       "      <th>cigsPerDay</th>\n",
       "      <th>BPMeds</th>\n",
       "      <th>prevalentStroke</th>\n",
       "      <th>prevalentHyp</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>totChol</th>\n",
       "      <th>sysBP</th>\n",
       "      <th>diaBP</th>\n",
       "      <th>BMI</th>\n",
       "      <th>heartRate</th>\n",
       "      <th>glucose</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>26.97</td>\n",
       "      <td>80.0</td>\n",
       "      <td>77.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>28.73</td>\n",
       "      <td>95.0</td>\n",
       "      <td>76.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>127.5</td>\n",
       "      <td>80.0</td>\n",
       "      <td>25.34</td>\n",
       "      <td>75.0</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>28.58</td>\n",
       "      <td>65.0</td>\n",
       "      <td>103.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>285.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>23.10</td>\n",
       "      <td>85.0</td>\n",
       "      <td>85.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   male  age  education  currentSmoker  cigsPerDay  BPMeds  prevalentStroke  \\\n",
       "0     1   39        4.0              0         0.0     0.0                0   \n",
       "1     0   46        2.0              0         0.0     0.0                0   \n",
       "2     1   48        1.0              1        20.0     0.0                0   \n",
       "3     0   61        3.0              1        30.0     0.0                0   \n",
       "4     0   46        3.0              1        23.0     0.0                0   \n",
       "\n",
       "   prevalentHyp  diabetes  totChol  sysBP  diaBP    BMI  heartRate  glucose  \n",
       "0             0         0    195.0  106.0   70.0  26.97       80.0     77.0  \n",
       "1             0         0    250.0  121.0   81.0  28.73       95.0     76.0  \n",
       "2             0         0    245.0  127.5   80.0  25.34       75.0     70.0  \n",
       "3             1         0    225.0  150.0   95.0  28.58       65.0    103.0  \n",
       "4             0         0    285.0  130.0   84.0  23.10       85.0     85.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = dataset.drop([\"TenYearCHD\"], axis=1)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    0\n",
       "3    1\n",
       "4    0\n",
       "Name: TenYearCHD, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = dataset[\"TenYearCHD\"]\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(X,y,test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_train = x_train.shape[0]\n",
    "m_test = x_test.shape[0]\n",
    "num_px = x_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3390,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For one example $x^{(i)}$:\n",
    "$$z^{(i)} = w^T x^{(i)} + b \\tag{1}$$\n",
    "$$\\hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)})\\tag{2}$$ \n",
    "$$ \\mathcal{L}(a^{(i)}, y^{(i)}) =  - y^{(i)}  \\log(a^{(i)}) - (1-y^{(i)} )  \\log(1-a^{(i)})\\tag{3}$$\n",
    "\n",
    "The cost is then computed by summing over all training examples:\n",
    "$$ J = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}(a^{(i)}, y^{(i)})\\tag{6}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    s = 1/(1+np.exp(-z))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid([1, 2]) = [0.73105858 0.88079708]\n"
     ]
    }
   ],
   "source": [
    "print (\"sigmoid([1, 2]) = \" + str(sigmoid(np.array([1,2]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialise params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise_with_zeros(dim):\n",
    "    w = np.zeros((dim,1))\n",
    "    b = 0\n",
    "    assert(w.shape == (dim,1))\n",
    "    assert(isinstance(b,float) or isinstance(b,int))\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 - Forward and Backward propagation\n",
    "\n",
    "Now that your parameters are initialized, you can do the \"forward\" and \"backward\" propagation steps for learning the parameters.\n",
    "\n",
    "**Exercise:** Implement a function `propagate()` that computes the cost function and its gradient.\n",
    "\n",
    "**Hints**:\n",
    "\n",
    "Forward Propagation:\n",
    "- You get X\n",
    "- You compute $A = \\sigma(w^T X + b) = (a^{(1)}, a^{(2)}, ..., a^{(m-1)}, a^{(m)})$\n",
    "- You calculate the cost function: $J = -\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)})$\n",
    "\n",
    "Here are the two formulas you will be using: \n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T\\tag{7}$$\n",
    "$$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})\\tag{8}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: propagate\n",
    "def propagate(w, b, X, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function and its gradient for the propagation explained above\n",
    "\n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n",
    "\n",
    "    Return:\n",
    "    cost -- negative log-likelihood cost for logistic regression\n",
    "    dw -- gradient of the loss with respect to w, thus same shape as w\n",
    "    db -- gradient of the loss with respect to b, thus same shape as b\n",
    "    \n",
    "    Tips:\n",
    "    - Write your code step by step for the propagation. np.log(), np.dot()\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # FORWARD PROPAGATION (FROM X TO COST)\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    A = sigmoid(np.dot(w.T,X) + b)              # compute activation\n",
    "    cost = np.sum(((- np.log(A))*Y + (-np.log(1-A))*(1-Y)))/m  # compute cost\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # BACKWARD PROPAGATION (TO FIND GRAD)\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    dw = (np.dot(X,(A-Y).T))/m\n",
    "    db = (np.sum(A-Y))/m\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    assert(dw.shape == w.shape)\n",
    "    assert(db.dtype == float)\n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return grads, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dw = [[0.99845601]\n",
      " [2.39507239]]\n",
      "db = 0.001455578136784208\n",
      "cost = 5.801545319394553\n"
     ]
    }
   ],
   "source": [
    "w, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]])\n",
    "grads, cost = propagate(w, b, X, Y)\n",
    "print (\"dw = \" + str(grads[\"dw\"]))\n",
    "print (\"db = \" + str(grads[\"db\"]))\n",
    "print (\"cost = \" + str(cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 - Optimization\n",
    "- You have initialized your parameters.\n",
    "- You are also able to compute a cost function and its gradient.\n",
    "- Now, you want to update the parameters using gradient descent.\n",
    "\n",
    "**Exercise:** Write down the optimization function. The goal is to learn $w$ and $b$ by minimizing the cost function $J$. For a parameter $\\theta$, the update rule is $ \\theta = \\theta - \\alpha \\text{ } d\\theta$, where $\\alpha$ is the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: optimize\n",
    "\n",
    "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n",
    "    \"\"\"\n",
    "    This function optimizes w and b by running a gradient descent algorithm\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of shape (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- True to print the loss every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    params -- dictionary containing the weights w and bias b\n",
    "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
    "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
    "    \n",
    "    Tips:\n",
    "    You basically need to write down two steps and iterate through them:\n",
    "        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n",
    "        2) Update the parameters using gradient descent rule for w and b.\n",
    "    \"\"\"\n",
    "    \n",
    "    costs = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        \n",
    "        # Cost and gradient calculation (≈ 1-4 lines of code)\n",
    "        ### START CODE HERE ### \n",
    "        grads, cost = propagate(w, b, X, Y)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Retrieve derivatives from grads\n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "        \n",
    "        # update rule (≈ 2 lines of code)\n",
    "        ### START CODE HERE ###\n",
    "        w = w - (learning_rate*dw)\n",
    "        b = b - (learning_rate*db)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Record the costs\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "        \n",
    "        # Print the cost every 100 training iterations\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    \n",
    "    params = {\"w\": w,\n",
    "              \"b\": b}\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return params, grads, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = [[-0.22504323]\n",
      " [-0.51983301]]\n",
      "b = 1.8727006195207803\n",
      "dw = [[0.07607736]\n",
      " [0.00195315]]\n",
      "db = -0.05298999640094985\n"
     ]
    }
   ],
   "source": [
    "params, grads, costs = optimize(w, b, X, Y, num_iterations= 300, learning_rate = 0.009, print_cost = False)\n",
    "\n",
    "print (\"w = \" + str(params[\"w\"]))\n",
    "print (\"b = \" + str(params[\"b\"]))\n",
    "print (\"dw = \" + str(grads[\"dw\"]))\n",
    "print (\"db = \" + str(grads[\"db\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f4c5f227e60>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: predict\n",
    "\n",
    "def predict(w, b, X):\n",
    "    '''\n",
    "    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (num_px * num_px * 3, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n",
    "    '''\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1,m))\n",
    "    w = w.reshape(X.shape[0], 1)\n",
    "    \n",
    "    # Compute vector \"A\" predicting the probabilities of a cat being present in the picture\n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    A = sigmoid(np.dot(w.T,X) + b)           # Dimentions = (1, m)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    #### WORKING SOLUTION 1: USING IF ELSE #### \n",
    "    for i in range(A.shape[1]):\n",
    "        # Convert probabilities A[0,i] to actual predictions p[0,i]\n",
    "        ## START CODE HERE ### (≈ 4 lines of code)\n",
    "        if (A[0,i] >= 0.5):\n",
    "           Y_prediction[0, i] = 1\n",
    "        else:\n",
    "           Y_prediction[0, i] = 0\n",
    "        ## END CODE HERE ###\n",
    "        \n",
    "    #### WORKING SOLUTION 2: ONE LINE ####\n",
    "    #for i in range(A.shape[1]):\n",
    "        ## Convert probabilities A[0,i] to actual predictions p[0,i]\n",
    "        ### START CODE HERE ### (≈ 4 lines of code)\n",
    "        #Y_prediction[0, i] = 1 if A[0,i] >=0.5 else 0\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    #### WORKING SOLUTION 3: VECTORISED IMPLEMENTATION ####\n",
    "    Y_prediction = (A >= 0.5) * 1.0\n",
    "    \n",
    "    assert(Y_prediction.shape == (1, m))\n",
    "    \n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions = [[1. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "w = np.array([[0.1124579],[0.23106775]])\n",
    "b = -0.3\n",
    "X = np.array([[1.,-1.1,-3.2],[1.2,2.,0.1]])\n",
    "print (\"predictions = \" + str(predict(w, b, X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: model\n",
    "\n",
    "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n",
    "    \"\"\"\n",
    "    Builds the logistic regression model by calling the function you've implemented previously\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n",
    "    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n",
    "    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n",
    "    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n",
    "    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n",
    "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
    "    print_cost -- Set to true to print the cost every 100 iterations\n",
    "    \n",
    "    Returns:\n",
    "    d -- dictionary containing information about the model.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # initialize parameters with zeros (≈ 1 line of code)\n",
    "    w, b = initialise_with_zeros(X_train.shape[0])\n",
    "\n",
    "    # Gradient descent (≈ 1 line of code)\n",
    "    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n",
    "    \n",
    "    # Retrieve parameters w and b from dictionary \"parameters\"\n",
    "    w = parameters[\"w\"]\n",
    "    b = parameters[\"b\"]\n",
    "    \n",
    "    # Predict test/train set examples (≈ 2 lines of code)\n",
    "    Y_prediction_test = predict(w, b, X_test)\n",
    "    Y_prediction_train = predict(w, b, X_train)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Print train/test Errors\n",
    "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
    "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
    "\n",
    "    \n",
    "    d = {\"costs\": costs,\n",
    "         \"Y_prediction_test\": Y_prediction_test, \n",
    "         \"Y_prediction_train\" : Y_prediction_train, \n",
    "         \"w\" : w, \n",
    "         \"b\" : b,\n",
    "         \"learning_rate\" : learning_rate,\n",
    "         \"num_iterations\": num_iterations}\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (3390, 15)\n",
      "Y_train shape: (3390,)\n",
      "X_test shape: (848, 15)\n",
      "Y_test shape: (848,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train shape:\", x_train.shape)\n",
    "print(\"Y_train shape:\", y_train.shape)\n",
    "print(\"X_test shape:\", x_test.shape)\n",
    "print(\"Y_test shape:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 85.07374631268436 %\n",
      "test accuracy: 83.72641509433961 %\n"
     ]
    }
   ],
   "source": [
    "# Convert to NumPy arrays and reshape Y_train and Y_test\n",
    "y_train = y_train.to_numpy().reshape(1, -1)\n",
    "y_test = y_test.to_numpy().reshape(1, -1)\n",
    "\n",
    "# Transpose X_train and X_test\n",
    "x_train = x_train.T\n",
    "x_test = x_test.T\n",
    "\n",
    "# Call the model function\n",
    "d = model(x_train, y_train, x_test, y_test, num_iterations=4000, learning_rate=0.005, print_cost=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
